{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e22d2bf",
   "metadata": {},
   "source": [
    "# Defining imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a808640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Third-party library imports\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.nn import TripletMarginLoss\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet50\n",
    "import torchvision.io as io\n",
    "import torchvision.transforms as transforms\n",
    "from typing import Tuple, List, Dict, Union, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf97024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matplotlib graph settings\n",
    "# plt.rcParams[\"savefig.bbox\"] = 'tight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8085e854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA (GPU support) is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eed0df7",
   "metadata": {},
   "source": [
    "# Loading ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcb6b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "ground_truth_df = pd.read_csv(\"ISIC2018_Task3_Test_GroundTruth.csv\", dtype={\"MEL\": int})\n",
    "\n",
    "# Rename the first column to \"Image\"\n",
    "ground_truth_df = ground_truth_df.rename(columns={ground_truth_df.columns[0]: \"Image\"})\n",
    "\n",
    "# Add '.jpg' to every value in the 'image' column\n",
    "ground_truth_df['Image'] = ground_truth_df['Image'] + '.jpg'\n",
    "\n",
    "# Extract the first two columns and rename the second column\n",
    "ground_truth_df = ground_truth_df.iloc[:, :2]  # Extracting first two columns\n",
    "ground_truth_df = ground_truth_df.rename(columns={\"MEL\": \"Ground truth labels\"})  # Renaming the second column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f895f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>Ground truth labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0034524.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0034525.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0034526.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ISIC_0034527.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ISIC_0034528.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>ISIC_0036060.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>ISIC_0036061.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>ISIC_0036062.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>ISIC_0036063.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>ISIC_0036064.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1512 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Image  Ground truth labels\n",
       "0     ISIC_0034524.jpg                    0\n",
       "1     ISIC_0034525.jpg                    0\n",
       "2     ISIC_0034526.jpg                    0\n",
       "3     ISIC_0034527.jpg                    0\n",
       "4     ISIC_0034528.jpg                    0\n",
       "...                ...                  ...\n",
       "1507  ISIC_0036060.jpg                    0\n",
       "1508  ISIC_0036061.jpg                    0\n",
       "1509  ISIC_0036062.jpg                    0\n",
       "1510  ISIC_0036063.jpg                    0\n",
       "1511  ISIC_0036064.jpg                    0\n",
       "\n",
       "[1512 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "display(ground_truth_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797232f",
   "metadata": {},
   "source": [
    "# Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bebd4b",
   "metadata": {},
   "source": [
    "**Defining custom model: Triplet Neural Network (with dataset class):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92996311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize PyTorch's data loading utilities\n",
    "class TripletDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    # Initialization\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        \n",
    "        # Initializes dataset and transformations\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Extract label from dataset\n",
    "        self.labels = [item[1] for item in dataset.imgs]\n",
    "        \n",
    "        # Create dictionary where keys are labels and values are lists of indices corresponding to each label.\n",
    "        self.label_to_indices = {label: np.where(np.array(self.labels) == label)[0]\n",
    "                                 for label in set(self.labels)}\n",
    "\n",
    "        \n",
    "    # Defines how individual items are retrieved from the dataset given an index (Called when dataset is indexed like dataset[index])\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # Extracts the image path and label of the anchor image at the given index from the dataset.\n",
    "        # label1 will be label of anchor/positive \n",
    "        img1, label1 = self.dataset.imgs[index]\n",
    "        \n",
    "        # Initialize positive index with the anchor index\n",
    "        positive_index = index\n",
    "        \n",
    "        # For positive index: randomly selects another index from the indices of images with the same label as the anchor image \n",
    "        while positive_index == index:\n",
    "            positive_index = np.random.choice(self.label_to_indices[label1])\n",
    "            \n",
    "        # For negative label: Randomly selects a label that is different from the label of the anchor image \n",
    "        negative_label = np.random.choice(list(set(self.labels) - set([label1])))\n",
    "        negative_index = np.random.choice(self.label_to_indices[negative_label])\n",
    "        \n",
    "        # Load images corresponding to the anchor, positive, and negative indices and convert images to RGB format\n",
    "        img2 = self.dataset.imgs[positive_index][0]\n",
    "        img3 = self.dataset.imgs[negative_index][0]\n",
    "        img1 = Image.open(img1).convert(\"RGB\")\n",
    "        img2 = Image.open(img2).convert(\"RGB\")\n",
    "        img3 = Image.open(img3).convert(\"RGB\")\n",
    "        \n",
    "        # If transformation is not None, apply the transformation\n",
    "        if self.transform is not None:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "            img3 = self.transform(img3)\n",
    "        \n",
    "        # Return images\n",
    "        return label1, img1, img2, img3\n",
    "\n",
    "    # Return the length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08b7e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4264a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNetwork(nn.Module):\n",
    "    def __init__(self, embedding_size=64):\n",
    "        super(TripletNetwork, self).__init__()\n",
    "        \n",
    "        # Load a pre-trained resnet50 model\n",
    "        self.backbone = resnet50(pretrained=True)\n",
    "        \n",
    "        # Replace the fully connected layer with an Identity module\n",
    "        self.backbone.fc = Identity()\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding_layer = nn.Linear(2048, embedding_size)  # Use 2048 as the in_features to match ResNet-50\n",
    "        \n",
    "        # Classification layers\n",
    "        self.fc = nn.Sequential(nn.ReLU(), nn.Dropout(0.7), nn.Linear(embedding_size, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        # Extract features using the backbone\n",
    "        x = self.backbone(x)  # This will now give a [batch_size, 2048] tensor directly\n",
    "        \n",
    "        # Get the embedding\n",
    "        embedding = self.embedding_layer(x)\n",
    "        \n",
    "        if return_embedding:\n",
    "            return embedding\n",
    "        \n",
    "        # Pass embedding through the classification layer\n",
    "        x = self.fc(embedding)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c70099",
   "metadata": {},
   "source": [
    "# Loading images into Data Loaders:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6770d19f",
   "metadata": {},
   "source": [
    "**Loading images into test data loader:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1a6e754",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageFolder(ImageFolder):\n",
    "    @staticmethod\n",
    "    def find_classes(directory: Union[str, Path]) -> Tuple[List[str], Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Finds the class folders in a dataset structured in a directory by overriding the sorting order.\n",
    "\n",
    "        Parameters:\n",
    "            directory (Union[str, Path]): Root directory path.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[str], Dict[str, int]]: (classes, class_to_idx) where classes are a list of \n",
    "                                              the class names and class_to_idx is a dictionary mapping \n",
    "                                              class name to class index.\n",
    "        \"\"\"\n",
    "        # Correct the syntax error by adding an extra set of parentheses around the generator expression\n",
    "        classes = sorted((entry.name for entry in os.scandir(directory) if entry.is_dir()), reverse=True)\n",
    "        if not classes:\n",
    "            raise FileNotFoundError(f\"Couldn't find any class folder in {directory}.\")\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "281b4002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set train and valid directory paths\n",
    "test_directory = 'data/Test'\n",
    "\n",
    "# Batch size\n",
    "bs = 32\n",
    "\n",
    "#define a standard transform to tensor\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Define the index mapping for folders to labels\n",
    "folder_to_label = {'no_melanoma': 0, 'melanoma': 1}\n",
    "\n",
    "# Load Data from folders\n",
    "data = {\n",
    "    'test': CustomImageFolder(root=test_directory, transform=transform)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f162a7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no_melanoma': 0, 'melanoma': 1, 'Test_All': 2}\n"
     ]
    }
   ],
   "source": [
    "print(data['test'].class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc0a87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TripletDataset(dataset=data['test'], transform=transform)\n",
    "test_data = DataLoader(test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26a79ba",
   "metadata": {},
   "source": [
    "# Predict One Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b843c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliaze the Triplet Network \n",
    "loaded_model = TripletNetwork(embedding_size=64) \n",
    "\n",
    "# Load the model from model_path\n",
    "loaded_model.load_state_dict(torch.load(\"model_weights.pth\"))\n",
    "\n",
    "# Set it in eval mode\n",
    "#loaded_model.eval()\n",
    "\n",
    "# Move model to device\n",
    "loaded_model = loaded_model.to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3793638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(model, image, reference_embeddings, device):\n",
    "    \n",
    "    # Switch model to evaluation mode\n",
    "    model.eval()  \n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Make sure the image has a batch dimension\n",
    "        if image.dim() == 3:\n",
    "            image = image.unsqueeze(0)  # Add batch dimension if not present\n",
    "        image = image.to(device)\n",
    "        \n",
    "        print(\"Input shape to model:\", image.shape)\n",
    "        \n",
    "        # Get the embedding of the uploaded image\n",
    "        image_embedding = model(image)\n",
    "\n",
    "    # Initialize the closest class and smallest distance\n",
    "    closest_class = None\n",
    "    smallest_distance = float('inf')\n",
    "\n",
    "    # Compare the uploaded image's embedding to each reference embedding\n",
    "    for class_name, ref_embedding in reference_embeddings.items():\n",
    "        distance = (image_embedding - ref_embedding.to(device)).pow(2).sum(1).item()\n",
    "        if distance < smallest_distance:\n",
    "            smallest_distance = distance\n",
    "            closest_class = class_name\n",
    "\n",
    "    return closest_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd5a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transform, device):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension and send to device\n",
    "    return image\n",
    "\n",
    "# Define the transform\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Example image path\n",
    "image_path = \"data/Test/melanoma/ISIC_0035914_v1.jpg\"\n",
    "\n",
    "# Extract image name only\n",
    "image_file_name = os.path.basename(image_path)\n",
    "#print(image_file_name)\n",
    "\n",
    "# Load the image\n",
    "image = load_image(image_path, transform, device)\n",
    "\n",
    "# Load embeddings\n",
    "reference_embeddings = torch.load('reference_embeddings.pt')\n",
    "\n",
    "# Now you can directly use the loaded image for prediction\n",
    "predicted_class = predict_class(loaded_model, image, reference_embeddings, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172dbc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the DataFrame\n",
    "display(ground_truth_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9467f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get ground truth label for a given image name\n",
    "def get_ground_truth(image_name):\n",
    "    \n",
    "    # Check if the image name exists in the DataFrame\n",
    "    if image_name in ground_truth_df['Image'].values:\n",
    "        \n",
    "        # Filter the DataFrame for the given image name and get the corresponding ground truth label\n",
    "        ground_truth_label = ground_truth_df.loc[ground_truth_df['Image'] == image_name, 'Ground truth labels'].iloc[0]\n",
    "        return ground_truth_label\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Image not found in DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5786a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_image_file_name = image_file_name.replace('_v1', '')\n",
    "#print(converted_image_file_name)\n",
    "\n",
    "actual_class = get_ground_truth(converted_image_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ea1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "if predicted_class == 1:\n",
    "    print(\"Image is predicted to contain Melanoma, Class 1\")\n",
    "else:\n",
    "    print(\"Image is predicted to not contain No Melanoma, Class 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0247dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "if actual_class == 1:\n",
    "    print(\"Ground-truth value for the Image is Melanoma, Class 1\")\n",
    "else:\n",
    "    print(\"Ground-truth value for the Image is No Melanoma, Class 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6298cb",
   "metadata": {},
   "source": [
    "# Predict Multiple Images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36a1dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageFolderPrediction(ImageFolder):\n",
    "    @staticmethod\n",
    "    def find_classes(directory: Union[str, os.PathLike]) -> Tuple[List[str], Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Finds the class folders in a dataset structured in a directory by overriding the sorting order.\n",
    "\n",
    "        Parameters:\n",
    "            directory (Union[str, os.PathLike]): Root directory path.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[List[str], Dict[str, int]]: (classes, class_to_idx) where classes are a list of \n",
    "                                              the class names sorted in reverse alphabetical order, \n",
    "                                              and class_to_idx is a dictionary mapping class name to class index.\n",
    "        \"\"\"\n",
    "        classes = sorted([entry.name for entry in os.scandir(directory) if entry.is_dir()], reverse=True)\n",
    "        class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        \"\"\"\n",
    "        Override the __getitem__ method to return the path along with the image and label.\n",
    "\n",
    "        Parameters:\n",
    "            index (int): Index of the item.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, label, path) where image is the transformed image tensor, label is the class label of the image, \n",
    "                   and path is the file path of the image.\n",
    "        \"\"\"\n",
    "        # Call the original __getitem__ to get the image and label\n",
    "        original_tuple = super(CustomImageFolderPrediction, self).__getitem__(index)\n",
    "        # Get the image path\n",
    "        path = self.imgs[index][0]  # self.imgs is a list of (image path, class index) tuples\n",
    "        # Return a new tuple that includes the original content plus the path\n",
    "        return original_tuple + (path,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcbb9ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_weights.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m TripletNetwork(embedding_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m) \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the model from model_path\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#loaded_model.load_state_dict(torch.load(\"model_weights_3_epoch_Triplet.pth\"))\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m loaded_model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_weights.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Set it in eval mode\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#loaded_model.eval()\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Move model to device\u001b[39;00m\n\u001b[0;32m     12\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model_weights.pth'"
     ]
    }
   ],
   "source": [
    "# Initiliaze the Triplet Network \n",
    "loaded_model = TripletNetwork(embedding_size=64) \n",
    "\n",
    "# Load the model from model_path\n",
    "#loaded_model.load_state_dict(torch.load(\"model_weights_3_epoch_Triplet.pth\"))\n",
    "loaded_model.load_state_dict(torch.load(\"saved_files/model_weights.pth\"))\n",
    "\n",
    "# Set it in eval mode\n",
    "#loaded_model.eval()\n",
    "\n",
    "# Move model to device\n",
    "loaded_model = loaded_model.to(device)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bb25b71",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'reference_embeddings.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m test_dataloader_predict \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset_predict, batch_size\u001b[38;5;241m=\u001b[39mbs, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load embeddings\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m reference_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreference_embeddings.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'reference_embeddings.pt'"
     ]
    }
   ],
   "source": [
    "test_dataset_predict = CustomImageFolderPrediction(root = test_directory, transform=transform)\n",
    "test_dataloader_predict = DataLoader(test_dataset_predict, batch_size=bs, shuffle=False)\n",
    "\n",
    "# Load embeddings\n",
    "reference_embeddings = torch.load('saved_files/reference_embeddings.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e85de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_classes(model, data_loader, reference_embeddings, device):\n",
    "    model.eval() \n",
    "    predictions = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _, paths in data_loader:  # Assuming paths are returned by the DataLoader\n",
    "            images = images.to(device)\n",
    "            image_embeddings = model(images)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                smallest_distance = float('inf')\n",
    "                closest_class = None\n",
    "                \n",
    "                for class_name, ref_embedding in reference_embeddings.items():\n",
    "                    distance = (image_embeddings[i] - ref_embedding.to(device)).pow(2).sum().item()\n",
    "                    if distance < smallest_distance:\n",
    "                        smallest_distance = distance\n",
    "                        closest_class = class_name\n",
    "                \n",
    "                filename = os.path.basename(paths[i])\n",
    "                predictions[filename] = closest_class\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f27870",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = predict_classes(loaded_model, test_dataloader_predict, reference_embeddings, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7604333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted class dataframe\n",
    "predicted_classes_df = pd.DataFrame(list(predicted_classes.items()), columns=['Image', 'Predicted Value'])\n",
    "\n",
    "# Remove '_v1' from the 'Filename' column\n",
    "predicted_classes_df['Image'] = predicted_classes_df['Image'].str.replace('_v1', '')\n",
    "\n",
    "# Display the DataFrame\n",
    "display(predicted_classes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d2a0d",
   "metadata": {},
   "source": [
    "**Merged dataframe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ca023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inner join on \"Image\" column\n",
    "merged_df = pd.merge(predicted_classes_df, ground_truth_df, on=\"Image\")\n",
    "\n",
    "# Display the merged DataFrame\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc28f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows where the Predicted Value is different from the Ground truth labels\n",
    "different_rows = merged_df[merged_df['Predicted Value'] != merged_df['Ground truth labels']]\n",
    "\n",
    "# Find rows where the Predicted Value is the same as the Ground truth labels\n",
    "same_rows = merged_df[merged_df['Predicted Value'] == merged_df['Ground truth labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb312229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the different rows\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(different_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89318953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the different rows\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(same_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of images uploaded were: {len(predicted_classes_df)}\")\n",
    "print(f\"The number of images classified correctly were {len(same_rows)}\")\n",
    "print(f\"The number of images classified incorrectly were {len(different_rows)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
